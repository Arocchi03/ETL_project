For our extract, transform, load exercise we analyzed two datasets from Kaggle. We pulled a Spotify CSV and a Billboard top 100 CSV to compare the hot 100 songs from 2019 and the number of streams. We stored the CSV's into our dataframe and then created new data with selected columns.

Before our transformation we confirmed the row count. Moving into the first piece of our transformation, we renamed columns in the billboard_df to match SQL schemata. We ran into some complexities where we then confirmed multiple instances of the same song in the Billboard rankings. Next we transformed from 97,225 rows to deduplicate 28,821 rows - now only one instance of each artist's track is shown in the database. Lastly, we confirmed that the transformations made were accurate by viewing unique values for each track where the weekly rank is now averaged for all weeks on the top 100.

Furthermore, prior to loading the dataframes to the postgres server, we created an engine connection to the local postgres and confirmed the tables. After the tables were confirmed and the dataframes were loaded into the postgres server, we confirmed that the data was available in the server to complete our ETL pipeline process.